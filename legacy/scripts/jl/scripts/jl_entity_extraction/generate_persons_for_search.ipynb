{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dictionary jl_persons_raw, create a set of entities for search\n",
    "import os, json, pickle, pprint\n",
    "root_data = \"~/jl/data/\"\n",
    "root = \"~/rovera/jl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_persons = pickle.load(open(root_data+\"person_names/jl_persons_raw.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "# 1 create correct name 'Homberg, Herz' --> 'Herz Homberg'\n",
    "# 2 remove single word names\n",
    "# 3 remove duplicates in the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "clean_persons = {k: set() for k in raw_persons.keys()}\n",
    "for k,v in raw_persons.items():\n",
    "    # 1\n",
    "    if \" \" in v['name'] and not ',' in v['name']: # ex. Aaron Samuel ben Moses Shalom\n",
    "        clean_persons[k].add(v['name'])\n",
    "        correct += 1\n",
    "    \n",
    "    elif ',' in v['name']: # ex Homberg, Herz\n",
    "        \n",
    "        chunks = v['name'].split(',')\n",
    "        if len(chunks) == 2:\n",
    "            plain_name = chunks[1].strip(' ') + \" \" + chunks[0].strip(' ')\n",
    "            clean_persons[k].add(plain_name)\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "    else:\n",
    "        wrong += 1\n",
    "        # do nothing ex 'Aron'\n",
    "        \n",
    "    for altn in v['altn']:\n",
    "        if ',' not in altn and ' ' in altn:\n",
    "            clean_persons[k].add(altn)\n",
    "            correct += 1\n",
    "        elif ',' in altn:\n",
    "            chunks = v['name'].split(',')\n",
    "            if len(chunks) == 2:\n",
    "                plain_name = chunks[1].strip(' ') + \" \" + chunks[0].strip(' ')\n",
    "                clean_persons[k].add(altn)\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg  2.098275066214333\n",
      "499945 122924\n"
     ]
    }
   ],
   "source": [
    "#sum(len(dct[key]) for dct in dataset.values())\n",
    "tot_hits = 0\n",
    "for k,v in clean_persons.items():\n",
    "    tot_hits += len(v)\n",
    "print(\"Avg \", tot_hits/len(clean_persons.keys()))\n",
    "print(correct, wrong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
