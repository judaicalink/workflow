{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, pickle, os, logging\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from rdflib import Graph, URIRef, Namespace\n",
    "from rdflib.namespace import RDF, OWL, RDFS\n",
    "import ep_manager as epm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = ['http://www.dnb.de/DE/Service/DigitaleDienste/EntityFacts/entityfacts_node.html']\n",
    "\n",
    "cwd = os.getcwd()\n",
    "if not os.path.exists(cwd+'/runs'):\n",
    "    os.mkdir(cwd+'/runs')\n",
    "if not os.path.exists(cwd+'/logs'):\n",
    "    os.mkdir(cwd+'/logs')\n",
    "\n",
    "ep_old_index, ep_old_inv_index, splitted_eps, merged_eps = {}, {}, {}, {}\n",
    "\n",
    "try:\n",
    "    os.rmdir(cwd+'/runs/.ipynb_checkpoints')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "latest = -1\n",
    "current = -1\n",
    "if len(os.listdir(cwd+'/runs')) != 0:\n",
    "    \n",
    "    for folder in os.listdir(cwd+'/runs'):\n",
    "        run = int(folder.split('_')[1])\n",
    "        if run > latest:\n",
    "            latest = run\n",
    "    ep_old_index = pickle.load(open(cwd+'/runs/run_'+str(latest)+'/ep_index.pickle', 'rb'))\n",
    "    ep_old_index = {k:v for k,v in ep_old_index.items() if len(v) > 0}\n",
    "    ep_old_inv_index = pickle.load(open(cwd+'/runs/run_'+str(latest)+'/ep_inv_index.pickle', 'rb'))\n",
    "    current = latest + 1\n",
    "    os.mkdir(cwd+'/runs/run_'+str(current))\n",
    "    \n",
    "else:\n",
    "    os.mkdir(cwd+'/runs/run_0')\n",
    "    current = 0\n",
    "\n",
    "ep_new_index = {ep: {} for ep in ep_old_index.keys()}\n",
    "ep_new_inv_index = {}\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename = cwd+'/logs/run_{}.log'.format(current),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "logging.info(\"Just set current RUN folders and loaded data structures...! Executing RUN {}.\".format(current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Performing queries...\")\n",
    "sparql = SPARQLWrapper(\"http://data.judaicalink.org/sparql/query\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    SELECT ?s ?same\n",
    "    WHERE { GRAPH ?g {\n",
    "        ?s owl:sameAs ?same\n",
    "        FILTER(!strstarts(str(?s), \"http://data.judaicalink.org/data/ep/\" ))\n",
    "        FILTER(!strstarts(str(?same), \"http://data.judaicalink.org/data/ep/\" ))\n",
    "        }}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "logging.info(\"Query 1: returned {} triples.\".format(len(results['results']['bindings'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6f55004b86bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\")\n\u001b[1;32m     16\u001b[0m \u001b[0msparql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetReturnFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msingleton_ents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Query 2: returned {} triples.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingleton_ents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bindings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/python/env/lib/python3.8/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0m_content_type_in_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SPARQL_JSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 \u001b[0m_validate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JSON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequestedFormat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convertJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0m_content_type_in_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_RDF_XML\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0m_validate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RDF/XML\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDFXML\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequestedFormat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/python/env/lib/python3.8/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36m_convertJSON\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \"\"\"\n\u001b[0;32m-> 1201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convertXML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper(\"http://data.judaicalink.org/sparql/query\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE { GRAPH ?g {\n",
    "        ?s ?p ?o .\n",
    "        \n",
    "        MINUS {\n",
    "            ?s owl:sameAs ?o .\n",
    "        }\n",
    "        }}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "singleton_ents = sparql.query().convert()\n",
    "logging.info(\"Query 2: returned {} triples.\".format(len(singleton_ents['results']['bindings'])))\n",
    "\n",
    "#FILTER (!strstarts(str(?s), \"http://data.judaicalink.org/data/compact-memory\"))\n",
    "#FILTER (!strstarts(str(?s), \"http://data.judaicalink.org/data/cm-tagme\"))\n",
    "#FILTER (!strstarts(str(?s), \"http://sammlungen.ub.uni-frankfurt.de/cm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_resources = set()\n",
    "for res in singleton_ents['results']['bindings']:\n",
    "    singleton_resources.add(res['s']['value'])\n",
    "\n",
    "    if res['o']['type'] != 'literal':\n",
    "        singleton_resources.add(res['o']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate temporary pools\n",
    "logging.info(\"Generating temporary pools of entities...\")\n",
    "\n",
    "t_index = {}\n",
    "t_inv_index = {}\n",
    "next_t_i = epm.get_next_index(t_index)\n",
    "\n",
    "for res in results['results']['bindings']:\n",
    "    s = res['s']['value']\n",
    "    same = res['same']['value']\n",
    "\n",
    "    if s not in blacklist and same not in blacklist:\n",
    "        if s in t_inv_index and same not in t_inv_index: # update existing pool\n",
    "            pool_i = t_inv_index[s]\n",
    "            t_index, t_inv_index = epm.update_pool(t_index, t_inv_index, pool_i, same)\n",
    "\n",
    "        elif same in t_inv_index and s not in t_inv_index: # update existing pool\n",
    "            pool_i = t_inv_index[same]\n",
    "            t_index, t_inv_index = epm.update_pool(t_index, t_inv_index, pool_i, s)\n",
    "\n",
    "        elif s not in t_inv_index and same not in t_inv_index: # standard case, create new pool\n",
    "            t_index, t_inv_index = epm.create_new_pool(t_index, t_inv_index, s, same, next_t_i)\n",
    "            next_t_i += 1\n",
    "            \n",
    "        else: # both uris are already in the index\n",
    "            s_pool = t_inv_index[s]\n",
    "            same_pool = t_inv_index[same]\n",
    "            \n",
    "            if s_pool != same_pool: # the two correponding pools are in fact the same --> MERGE\n",
    "                t_index, t_inv_index = epm.merge_pools(s_pool, same_pool, t_index, t_inv_index, next_t_i)\n",
    "                next_t_i += 1\n",
    "\n",
    "# create singleton pools\n",
    "for res in singleton_resources:\n",
    "    if res not in t_inv_index:\n",
    "        t_index, t_inv_index = epm.create_new_singleton_pool(t_index, t_inv_index, res, next_t_i)\n",
    "        next_t_i += 1\n",
    "    \n",
    "logging.info(\"Done! Created {} pools for {} URIs.\".format(len(t_index), len(t_inv_index)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity pages from computed entity pools\n",
    "logging.info(\"Creating entity pages from computed entity pools...\")\n",
    "\n",
    "# track activated case history\n",
    "case_tracker = {\n",
    "    'CREATE NEW': 0,\n",
    "    'COPY': 0,\n",
    "    'MERGE': 0,\n",
    "    'SPLIT': 0,\n",
    "    'UPDATE': 0\n",
    "}\n",
    "\n",
    "next_ep_id = epm.get_next_ep_index(ep_new_index)\n",
    "\n",
    "for t_cluster in t_index.values():\n",
    "    \n",
    "    # create mapping between current cluster and previous (old) EP index\n",
    "    mapping = {}\n",
    "    diff = set([])\n",
    "    for resource in t_cluster:\n",
    "        try:\n",
    "            old_ep = ep_old_inv_index[resource]\n",
    "            if old_ep not in mapping:\n",
    "                mapping[old_ep] = set([])\n",
    "            mapping[old_ep].add(resource)\n",
    "        except KeyError:\n",
    "            diff.add(resource)\n",
    "  \n",
    "\n",
    "    if len(mapping) == 0:\n",
    "        # create new entity page\n",
    "        new_ep_URI = \"http://data.judaicalink.org/data/ep/\"+str(next_ep_id)\n",
    "        next_ep_id += 1\n",
    "        ep_new_index[new_ep_URI] = t_cluster\n",
    "        for resource in t_cluster:\n",
    "            ep_new_inv_index[resource] = new_ep_URI\n",
    "        \n",
    "        case_tracker['CREATE NEW'] += 1\n",
    "    \n",
    "    elif len(mapping) == 1: # current cluster maps to exactly one old entity page\n",
    "        for ep_uri, ep_pool in mapping.items():\n",
    "            old_ep_pool = ep_old_index[ep_uri]\n",
    "            \n",
    "            if old_ep_pool == ep_pool:\n",
    "                # Copy the old ep in new index or update if t_cluster contains new (never seen) resources\n",
    "                ep_new_index[ep_uri] = t_cluster\n",
    "                for resource in t_cluster:\n",
    "                    ep_new_inv_index[resource] = ep_uri\n",
    "                \n",
    "                if len(diff) == 0:\n",
    "                    case_tracker['COPY'] += 1\n",
    "                else:\n",
    "                    case_tracker['UPDATE'] += 1\n",
    "                    \n",
    "            elif ep_pool.issubset(old_ep_pool):\n",
    "                # Split old ep\n",
    "                new_ep_URI = \"http://data.judaicalink.org/data/ep/\"+str(next_ep_id)\n",
    "                next_ep_id += 1\n",
    "\n",
    "                if ep_uri not in splitted_eps:\n",
    "                    splitted_eps[ep_uri] = set()\n",
    "                splitted_eps[ep_uri].add(new_ep_URI)\n",
    "\n",
    "                ep_new_index[new_ep_URI] = t_cluster\n",
    "                for resource in t_cluster:\n",
    "                    ep_new_inv_index[resource] = new_ep_URI\n",
    "                case_tracker['SPLIT'] += 1\n",
    "                case_tracker['CREATE NEW'] += 1\n",
    "    \n",
    "    elif len(mapping) > 1: # current cluster maps to more than one old entity pages\n",
    "        \n",
    "        new_ep_URI = \"http://data.judaicalink.org/data/ep/\"+str(next_ep_id)\n",
    "        next_ep_id += 1\n",
    "        \n",
    "        for ep_uri, ep_pool in mapping.items():\n",
    "            old_ep_pool = ep_old_index[ep_uri]\n",
    "            \n",
    "            if ep_pool == old_ep_pool:\n",
    "                # merge old ep into new\n",
    "                merged_eps[ep_uri] = new_ep_URI\n",
    "                case_tracker['MERGE'] += 1\n",
    "            elif ep_pool.issubset(old_ep_pool):\n",
    "                # split old ep\n",
    "                if ep_uri not in splitted_eps:\n",
    "                    splitted_eps[ep_uri] = set()\n",
    "                splitted_eps[ep_uri].add(new_ep_URI)\n",
    "                case_tracker['SPLIT'] += 1\n",
    "        \n",
    "        ep_new_index[new_ep_URI] = t_cluster\n",
    "        for resource in t_cluster:\n",
    "            ep_new_inv_index[resource] = new_ep_URI  \n",
    "        case_tracker['CREATE NEW'] += 1\n",
    "\n",
    "ep_new_index = {k:v for k,v in ep_new_index.items() if len(v) != 0}\n",
    "logging.info(\"Done! Generated {} entity pages.\".format(len(ep_new_index)))\n",
    "logging.info(\"Case tracker: {}\".format(case_tracker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Saving outputs to disk...\")\n",
    "with open(cwd+'/runs/run_'+str(current)+'/splitted_eps.pickle', 'wb') as outfile:\n",
    "    pickle.dump(splitted_eps, outfile)\n",
    "with open(cwd+'/runs/run_'+str(current)+'/merged_eps.pickle', 'wb') as outfile:\n",
    "    pickle.dump(merged_eps, outfile)\n",
    "\n",
    "with open(cwd+'/runs/run_'+str(current)+'/ep_old_index.pickle', 'wb') as outfile:\n",
    "    pickle.dump(ep_old_index, outfile)\n",
    "with open(cwd+'/runs/run_'+str(current)+'/ep_old_inv_index.pickle', 'wb') as outfile:\n",
    "    pickle.dump(ep_old_inv_index, outfile)\n",
    "\n",
    "with open(cwd+'/runs/run_'+str(current)+'/ep_index.pickle', 'wb') as outfile:\n",
    "    pickle.dump(ep_new_index, outfile)\n",
    "with open(cwd+'/runs/run_'+str(current)+'/ep_inv_index.pickle', 'wb') as outfile:\n",
    "    pickle.dump(ep_new_inv_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generating RDF graph...\")\n",
    "ep_g = epm.generate_ep_graph_sep(ep_new_index, save=True, return_graph=True, outpath=cwd+\"/runs/run_\"+str(current)+'/entity_pages.ttl', rdf_format='ttl')\n",
    "logging.info(\"Generated {} triples.\".format(len(ep_g)))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
