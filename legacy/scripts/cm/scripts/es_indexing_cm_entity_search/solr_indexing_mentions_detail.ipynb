{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skript to build the SOLR Core for cm_entities with details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pprint, csv, pickle, pysolr, time, sys, requests\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/data/scripts/cm/scripts/\"\n",
    "jl_output_path = \"/data/scripts/jl/data/\"\n",
    "SOLR_URL = \"http://localhost:8983/solr\"\n",
    "CORE_NAME = \"cm_entities_beta\"\n",
    "CONFIG_SET = \"_default\"\n",
    "CHUNK_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 entity name-uri mapping\n",
    "name_to_uri = json.load(open(root_path + 'cooccurrence/output/name_to_uri.json', 'r', encoding=\"utf-8\"))\n",
    "uri_to_name = json.load(open(root_path + 'cooccurrence/output/uri_to_name.json', 'r', encoding=\"utf-8\"))\n",
    "# 2 entity pages inverted index\n",
    "ep_inv_index = pickle.load(open(jl_output_path + 'entity_pages/ep_inv_index.pickle', 'rb'))\n",
    "# 3 occurrence by journal\n",
    "occurrence_by_journal = pickle.load(open(root_path + 'cooccurrence/output/occ_by_journal_detail.pickle', 'rb'))\n",
    "# 4 related entities\n",
    "entity_correlation = pickle.load(open(root_path + 'cooccurrence/output/entity_correlation.pickle', 'rb'))\n",
    "# 5 data types \n",
    "classified_entities = json.load(open(root_path + 'jl_linking/classified_entities.json', 'r', encoding=\"utf-8\"))\n",
    "# 6 TAGME mentions\n",
    "#cm_entities = pickle.load(open(root_path + 'cooccurrence/cm_tagme_resource_reference_data_05_03.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3256\n",
      "chunk 1 3256\n",
      "chunk 2 3256\n",
      "chunk 3 3256\n",
      "chunk 4 3256\n",
      "chunk 5 3256\n",
      "chunk 6 3256\n",
      "chunk 7 3256\n",
      "chunk 8 3256\n",
      "chunk 9 3256\n",
      "chunk 10 3256\n",
      "chunk 11 3256\n",
      "chunk 12 3256\n",
      "chunk 13 3256\n",
      "chunk 14 3256\n",
      "chunk 15 3256\n",
      "chunk 16 3256\n",
      "chunk 17 3256\n",
      "chunk 18 3256\n",
      "chunk 19 3256\n",
      "chunk 20 3256\n",
      "chunk 21 3256\n",
      "chunk 22 3256\n",
      "chunk 23 3256\n",
      "chunk 24 3256\n",
      "chunk 25 3256\n",
      "chunk 26 3256\n",
      "chunk 27 3256\n",
      "chunk 28 3256\n",
      "chunk 29 3256\n",
      "chunk 30 54\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "i,j,chunk = 0,0,0\n",
    "for name, uri in name_to_uri.items():\n",
    "    i += 1\n",
    "    j += 1\n",
    "    occs_by_journal = []\n",
    "    for journal_info, mentions in occurrence_by_journal[uri].items():\n",
    "        journal_mentions = {'j_name': journal_info[1],\n",
    "                                'j_id': journal_info[0],\n",
    "                                'first': mentions['first'],\n",
    "                                'last': mentions['last'],\n",
    "                               'mentions': mentions['data']\n",
    "                               }\n",
    "        occs_by_journal.append(journal_mentions)\n",
    "\n",
    "\n",
    "    occs_by_journal = sorted(occs_by_journal, key=lambda x:len(x['mentions']), reverse=True)\n",
    "    related_entities = sorted([[res_uri, uri_to_name[res_uri], score] for res_uri, score in entity_correlation[uri].items()], key=lambda x:x[2], reverse=True)\n",
    "\n",
    "    # add entity type information to related entities\n",
    "    for rel_ent in related_entities:\n",
    "        ent_type = classified_entities[rel_ent[1].replace(' ', '_')]\n",
    "        rel_ent.append(ent_type)\n",
    "\n",
    "\n",
    "    label_name = name.replace(' ', '_')\n",
    "    e_type = classified_entities[label_name]\n",
    "    if len(e_type) == 0:\n",
    "        e_type = \"OTH\"\n",
    "\n",
    "\n",
    "    header = {'_id': str(j), '_index': 'cm_entities'}\n",
    "    body = {'name': name,\n",
    "            'e_type': e_type,\n",
    "            'journal_occs': occs_by_journal,\n",
    "            'related_entities': related_entities,\n",
    "            'ep': ''\n",
    "           }\n",
    "    # to be fixed: not all entity pages exist!\n",
    "    if uri in ep_inv_index:\n",
    "        body['ep'] = ep_inv_index[uri]\n",
    "\n",
    "\n",
    "    data.append(json.dumps({\"index\": header}, ensure_ascii=False))\n",
    "    data.append(json.dumps(body, ensure_ascii=False))\n",
    "\n",
    "    if i == 200:\n",
    "        print(sys.getsizeof(data))\n",
    "        with open('/data/cm/output/temp_journals_detail/chunk_'+str(chunk)+'.json', \"w\", encoding=\"utf-8\") as write_file:\n",
    "            write_file.write(\"\\n\".join(data))\n",
    "        data = []\n",
    "        i = 0\n",
    "        chunk += 1\n",
    "        print(\"chunk\", chunk, end=\" \")\n",
    "\n",
    "    \n",
    "with open('/data/cm/output/temp_journals_detail/chunk_'+str(chunk)+'.json', \"w\", encoding=\"utf-8\") as write_file:\n",
    "    write_file.write(\"\\n\".join(data))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define mappings\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        'name': {'type': 'text'},\n",
    "        'ep': {'type': 'text'},\n",
    "        'e_type': {'type': 'text'},\n",
    "        'journal_occs': {'type': 'nested',\n",
    "                        'properties': {\n",
    "                            'j_name': {'type': 'text'},\n",
    "                            'j_id': {'type': 'text'},\n",
    "                            'first': {'type': 'integer'},\n",
    "                            'last': {'type': 'integer'},\n",
    "                            'mentions': {\n",
    "                                'type': 'nested',\n",
    "                                'properties': {\n",
    "                                    'p_id': {'type': 'text'},\n",
    "                                    'spot': {'type': 'text'},\n",
    "                                    'start': {'type': 'integer'},\n",
    "                                    'end': {'type': 'integer'},\n",
    "                                    'p_link': {'type': 'text'},\n",
    "                                    'date': {'type': 'text'}\n",
    "                                } \n",
    "                            }\n",
    "                        }\n",
    "                        },\n",
    "        'related_entities': {'type': 'text'}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the SOLR Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "def solr_core_exists(solr_url: str, core_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a given Solr core exists using the Core Admin STATUS action.\n",
    "    \"\"\"\n",
    "    status_url = f\"{solr_url}/admin/cores\"\n",
    "    params = {\n",
    "        \"action\": \"STATUS\",\n",
    "        \"core\": core_name,\n",
    "        \"wt\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(status_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # If the core exists, it should appear in the \"status\" dict with some content\n",
    "    status = data.get(\"status\", {})\n",
    "    return core_name in status and bool(status[core_name])\n",
    "\n",
    "\n",
    "def solr_delete_core(solr_url: str, core_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Delete (unload) a Solr core and remove its index, data and instance directory.\n",
    "    \"\"\"\n",
    "    unload_url = f\"{solr_url}/admin/cores\"\n",
    "    params = {\n",
    "        \"action\": \"UNLOAD\",\n",
    "        \"core\": core_name,\n",
    "        \"deleteIndex\": \"true\",\n",
    "        \"deleteDataDir\": \"true\",\n",
    "        \"deleteInstanceDir\": \"true\",\n",
    "        \"wt\": \"json\",\n",
    "    }\n",
    "\n",
    "    print(f\"Unloading and deleting core '{core_name}' …\")\n",
    "    response = requests.get(unload_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Core '{core_name}' successfully unloaded and deleted.\")\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "def solr_create_core(solr_url: str, core_name: str, config_set: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a Solr core using a given configSet.\n",
    "    \"\"\"\n",
    "    create_url = f\"{solr_url}/admin/cores\"\n",
    "    params = {\n",
    "        \"action\": \"CREATE\",\n",
    "        \"name\": core_name,\n",
    "        \"configSet\": config_set,\n",
    "        \"wt\": \"json\",\n",
    "    }\n",
    "\n",
    "    print(f\"Creating core '{core_name}' with configSet '{config_set}' …\")\n",
    "    response = requests.get(create_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Core '{core_name}' successfully created.\")\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "def recreate_solr_core(solr_url: str, core_name: str, config_set: str) -> None:\n",
    "    \"\"\"\n",
    "    Check if the core exists, delete it if necessary, and create it again.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if solr_core_exists(solr_url, core_name):\n",
    "            print(f\"Core '{core_name}' already exists.\")\n",
    "            solr_delete_core(solr_url, core_name)\n",
    "            # Small delay to give Solr time to clean up\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print(f\"Core '{core_name}' does not exist yet.\")\n",
    "\n",
    "        solr_create_core(solr_url, core_name, config_set)\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Error while communicating with Solr:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core 'cm_entities_beta' already exists.\n",
      "Unloading and deleting core 'cm_entities_beta' …\n",
      "Core 'cm_entities_beta' successfully unloaded and deleted.\n",
      "{'responseHeader': {'status': 0, 'QTime': 179}}\n",
      "Creating core 'cm_entities_beta' with configSet '_default' …\n",
      "Core 'cm_entities_beta' successfully created.\n",
      "{'responseHeader': {'status': 0, 'QTime': 231}, 'core': 'cm_entities_beta'}\n"
     ]
    }
   ],
   "source": [
    "recreate_solr_core(SOLR_URL, CORE_NAME, CONFIG_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"index\": {\"_id\": \"6001\", \"_index\": \"cm_entities\"}}', '{\"name\": \"Sigtuna\", \"e_type\": \"LOC\", \"journal_occs\": [{\"j_name\": \"Israelitische Wochenschrift für die religiösen und socialen Interessen des Judenthums\", \"j_id\": \"5369809\", \"first\": 1877, \"last\": 1877, \"mentions\": [{\"p_id\": \"9583660\", \"spot\": \"Sigtuna\", \"start\": 3413, \"end\": 3420, \"p_link\": \"9583660\", \"date\": \"1877-08-15\", \"year\": 1877}]}], \"related_entities\": [[\"http://data.judaicalink.org/data/dbpedia/Stockholm\", \"Stockholm\", 0.00015971889474524837, \"LOC\"]], \"ep\": \"http://data.judaicalink.org/data/ep/1924433\"}', '{\"index\": {\"_id\": \"6002\", \"_index\": \"cm_entities\"}}', '{\"name\": \"Victor Hollaender\", \"e_type\": \"PER\", \"journal_occs\": [{\"j_name\": \"Israelitische Wochenschrift für die religiösen und socialen Interessen des Judenthums\", \"j_id\": \"5369809\", \"first\": 1905, \"last\": 1905, \"mentions\": [{\"p_id\": \"9607067\", \"spot\": \"Victor Hollaender\", \"start\": 7630, \"end\": 7647, \"p_link\": \"9607067\", \"date\": \"1905-11-24\", \"year\": 1905}]}], \"related_entities\": [], \"ep\": \"http://data.judaicalink.org/data/ep/1244481\"}', '{\"index\": {\"_id\": \"6003\", \"_index\": \"cm_entities\"}}']\n"
     ]
    }
   ],
   "source": [
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nested_docs_from_bulk_lines(lines: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Transform Elasticsearch bulk-like NDJSON into Solr nested documents\n",
    "    with three levels:\n",
    "      - parent: entity\n",
    "      - child: journal occurrence\n",
    "      - grandchild: mentions within a journal\n",
    "      - sibling children: related entities\n",
    "\n",
    "    Expected input (alternating lines):\n",
    "      {\"index\": {\"_id\": \"0\", \"_index\": \"cm_entities\"}}\n",
    "      {\n",
    "        \"name\": \"...\",\n",
    "        \"e_type\": \"...\",\n",
    "        \"journal_occs\": [\n",
    "          {\n",
    "            \"j_name\": \"...\",\n",
    "            \"j_id\": \"...\",\n",
    "            \"first\": 123,\n",
    "            \"last\": 456,\n",
    "            \"mentions\": [\n",
    "              {\n",
    "                \"p_id\": \"...\",\n",
    "                \"spot\": \"...\",\n",
    "                \"start\": 10,\n",
    "                \"end\": 20,\n",
    "                \"p_link\": \"...\",\n",
    "                \"date\": \"YYYY-MM-DD\"\n",
    "              },\n",
    "              ...\n",
    "            ]\n",
    "          },\n",
    "          ...\n",
    "        ],\n",
    "        \"related_entities\": [\n",
    "           [rel_uri, rel_name, score, ent_type],\n",
    "           ...\n",
    "        ],\n",
    "        \"ep\": \"...\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    docs: List[Dict[str, Any]] = []\n",
    "    current_id: str | None = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        # Meta line from Elasticsearch bulk\n",
    "        if \"index\" in obj:\n",
    "            current_id = str(obj[\"index\"][\"_id\"])\n",
    "            continue\n",
    "\n",
    "        if current_id is None:\n",
    "            # No index line before – skip or raise, here we skip\n",
    "            continue\n",
    "\n",
    "        body = obj\n",
    "        parent_id = current_id  # you can change this to body[\"ep\"] or similar if you prefer\n",
    "\n",
    "        # --- Parent document (entity) ---\n",
    "        parent_doc: Dict[str, Any] = {\n",
    "            \"id\": parent_id,\n",
    "            \"name\": body.get(\"name\"),\n",
    "            \"e_type\": body.get(\"e_type\"),\n",
    "            \"ep\": body.get(\"ep\"),\n",
    "        }\n",
    "\n",
    "        child_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "        # --- Journal occurrences (children) ---\n",
    "        for j_idx, journal_occ in enumerate(body.get(\"journal_occs\", [])):\n",
    "            j_name = journal_occ.get(\"j_name\")\n",
    "            j_id = journal_occ.get(\"j_id\")\n",
    "            first = journal_occ.get(\"first\")\n",
    "            last = journal_occ.get(\"last\")\n",
    "            mentions = journal_occ.get(\"mentions\", [])\n",
    "\n",
    "            # Grandchildren: individual mentions in this journal\n",
    "            mention_children: List[Dict[str, Any]] = []\n",
    "            for m_idx, mention in enumerate(mentions):\n",
    "                mention_children.append(\n",
    "                    {\n",
    "                        # every child doc needs an id because Solr's uniqueKey is required\n",
    "                        \"id\": f\"{parent_id}_j_{j_idx}_m_{m_idx}\",\n",
    "                        \"type\": \"mention\",\n",
    "                        \"p_id\": mention.get(\"p_id\"),\n",
    "                        \"spot\": mention.get(\"spot\"),\n",
    "                        \"start\": int(mention.get(\"start\")) if mention.get(\"start\") is not None else None,\n",
    "                        \"end\": int(mention.get(\"end\")) if mention.get(\"end\") is not None else None,\n",
    "                        \"p_link\": mention.get(\"p_link\"),\n",
    "                        \"date\": mention.get(\"date\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            journal_doc: Dict[str, Any] = {\n",
    "                \"id\": f\"{parent_id}_j_{j_idx}\",\n",
    "                \"type\": \"journal_occurrence\",\n",
    "                \"j_name\": j_name,\n",
    "                \"j_id\": j_id,\n",
    "                \"first\": int(first) if first is not None else None,\n",
    "                \"last\": int(last) if last is not None else None,\n",
    "            }\n",
    "\n",
    "            if mention_children:\n",
    "                journal_doc[\"_childDocuments_\"] = mention_children\n",
    "\n",
    "            child_docs.append(journal_doc)\n",
    "\n",
    "        # --- Related entities (children on same level as journals) ---\n",
    "        for r_idx, rel in enumerate(body.get(\"related_entities\", [])):\n",
    "            # rel = [rel_uri, rel_name, score, ent_type]\n",
    "            if len(rel) < 4:\n",
    "                # safety: skip malformed entries\n",
    "                continue\n",
    "            rel_uri, rel_name, score, rel_type = rel\n",
    "            child_docs.append(\n",
    "                {\n",
    "                    \"id\": f\"{parent_id}_r_{r_idx}\",\n",
    "                    \"type\": \"related_entity\",\n",
    "                    \"rel_uri\": rel_uri,\n",
    "                    \"rel_name\": rel_name,\n",
    "                    \"rel_score\": float(score),\n",
    "                    \"rel_e_type\": rel_type,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if child_docs:\n",
    "            parent_doc[\"_childDocuments_\"] = child_docs\n",
    "\n",
    "        docs.append(parent_doc)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_chunks(items, size: int):\n",
    "    \"\"\"Yield successive chunks of `size` from list `items`.\"\"\"\n",
    "    for i in range(0, len(items), size):\n",
    "        yield items[i : i + size]\n",
    "\n",
    "\n",
    "def upload_nested_docs_to_solr(\n",
    "    docs: List[Dict[str, Any]],\n",
    "    solr_url: str = SOLR_URL,\n",
    "    core_name: str = CORE_NAME,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload nested documents (parent + children) to Solr in chunks using pysolr.\n",
    "    \"\"\"\n",
    "    solr = pysolr.Solr(f\"{solr_url}/{core_name}\", timeout=200)\n",
    "\n",
    "    total = len(docs)\n",
    "    print(f\"Indexing {total} parent docs (with children) into '{core_name}' …\")\n",
    "\n",
    "    start = time.time()\n",
    "    for idx, chunk in enumerate(iter_chunks(docs, chunk_size), start=1):\n",
    "        print(f\"  Sending chunk {idx} ({len(chunk)} parent docs)…\")\n",
    "        solr.add(chunk, commit=False)\n",
    "\n",
    "    solr.commit()\n",
    "    end = time.time()\n",
    "    print(f\"Finished indexing in {end - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_docs = build_nested_docs_from_bulk_lines(data)\n",
    "upload_nested_docs_to_solr(nested_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data/cm/output/temp_journals_detail/chunk_0.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 29.12 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_1.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 6.06 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_10.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 1.31 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_11.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 1.17 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_12.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.84 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_13.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.75 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_14.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 1.19 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_15.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.94 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_16.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.76 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_17.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.77 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_18.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.82 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_19.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 1.09 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_2.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 7.17 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_20.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.66 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_21.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.45 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_22.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.42 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_23.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.37 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_24.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.41 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_25.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.39 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_26.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.17 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_27.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.16 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_28.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.20 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_29.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 0.15 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_3.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 3.79 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_30.json …\n",
      "Indexing 54 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (54 parent docs)…\n",
      "Finished indexing in 0.15 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_4.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 3.14 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_5.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 3.23 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_6.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 2.53 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_7.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 2.55 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_8.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 2.82 seconds.\n",
      "Processing /data/cm/output/temp_journals_detail/chunk_9.json …\n",
      "Indexing 200 parent docs (with children) into 'cm_entities_beta' …\n",
      "  Sending chunk 1 (200 parent docs)…\n",
      "Finished indexing in 1.83 seconds.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "chunk_dir = \"/data/cm/output/temp_journals_detail\"\n",
    "\n",
    "# optional sort for reproducible order\n",
    "chunk_files = sorted(glob.glob(os.path.join(chunk_dir, \"chunk_*.json\")))\n",
    "\n",
    "for fname in chunk_files:\n",
    "    print(f\"Processing {fname} …\")\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "    nested_docs = build_nested_docs_from_bulk_lines(lines)\n",
    "    upload_nested_docs_to_solr(nested_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search took 0.04\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "solr = pysolr.Solr(f\"{SOLR_URL}/{CORE_NAME}\", always_commit=False, timeout=10)\n",
    "start = time.time()\n",
    "res = solr.search('name:Israel', index=CORE_NAME, rows=10000)\n",
    "end = time.time()\n",
    "print(\"Search took\", round(end-start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(res.hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '18', 'name': ['Israel'], 'e_type': ['LOC'], 'ep': ['http://data.judaicalink.org/data/ep/1188573'], '_version_': 1850522079782240256}\n",
      "{'id': '2166', 'name': ['Israel Friedlaender'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1265162'], '_version_': 1850522111931580416}\n",
      "{'id': '2874', 'name': ['Israel Jacobson'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1005126'], '_version_': 1850522116434165760}\n",
      "{'id': '2219', 'name': ['Israel Belkind'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1005119'], '_version_': 1850522113038876673}\n",
      "{'id': '5466', 'name': ['Israel Gutman'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1005123'], '_version_': 1850522136200871940}\n",
      "{'id': '5492', 'name': ['Israel Nadschara'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/2598311'], '_version_': 1850522136206114821}\n",
      "{'id': '5804', 'name': ['Israel Aksenfeld'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1356530'], '_version_': 1850522136580456448}\n",
      "{'id': '5880', 'name': ['Uwe Israel'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1114134'], '_version_': 1850522136591990789}\n",
      "{'id': '5923', 'name': ['Jonathan Israel'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1005366'], '_version_': 1850522136596185093}\n",
      "{'id': '2674', 'name': ['Israel Salanter'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1197635'], '_version_': 1850522115426484224}\n",
      "{'id': '2483', 'name': ['Israel Isserlein'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1002899'], '_version_': 1850522114368471041}\n",
      "{'id': '4121', 'name': ['Joachim Israel'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1171385'], '_version_': 1850522132806631425}\n",
      "{'id': '4203', 'name': ['Israel Brodie'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1002557'], '_version_': 1850522133450457089}\n",
      "{'id': '4363', 'name': ['Israel Sarug'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1005133'], '_version_': 1850522133537488896}\n",
      "{'id': '616', 'name': ['Israel Zangwill'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1845025'], '_version_': 1850522137947799552}\n",
      "{'id': '695', 'name': ['Israel Bruna'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1002740'], '_version_': 1850522139028881408}\n",
      "{'id': '825', 'name': ['Adonia (Israel)'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1756108'], '_version_': 1850522142617108480}\n",
      "{'id': '935', 'name': ['Israel Brandmann'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1124507'], '_version_': 1850522143906856960}\n",
      "{'id': '1154', 'name': ['Israel Alter'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1005117'], '_version_': 1850522148634886144}\n",
      "{'id': '1442', 'name': ['James Israel'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/2312481'], '_version_': 1850522154320265216}\n",
      "{'id': '1558', 'name': ['Israel Friedmann'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1396283'], '_version_': 1850522155420221440}\n",
      "{'id': '2184', 'name': ['Israel Chaim Tawiow'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1148632'], '_version_': 1850522112018612224}\n",
      "{'id': '2384', 'name': ['Israel Meir Lau'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/2155307'], '_version_': 1850522113717305344}\n",
      "{'id': '4649', 'name': ['Meyer Israel Bresselau'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1006066'], '_version_': 1850522134699311107}\n",
      "{'id': '5802', 'name': ['Moses Israel Fürst'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1006232'], '_version_': 1850522136579407872}\n",
      "{'id': '3401', 'name': ['Israel Meir Freimann'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1002979'], '_version_': 1850522120011907072}\n",
      "{'id': '3642', 'name': ['Israel Isidor Eljaschoff'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/2655869'], '_version_': 1850522121215672321}\n",
      "{'id': '4409', 'name': ['Israel Meir Kagan'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/1530537'], '_version_': 1850522134130982912}\n",
      "{'id': '1033', 'name': ['Menasse ben Israel'], 'e_type': ['OTH'], 'ep': ['http://data.judaicalink.org/data/ep/1399058'], '_version_': 1850522147146956800}\n",
      "{'id': '1398', 'name': ['Israel ben Elieser'], 'e_type': ['PER'], 'ep': ['http://data.judaicalink.org/data/ep/2296305'], '_version_': 1850522152663515136}\n"
     ]
    }
   ],
   "source": [
    "for doc in res:\n",
    "    print(doc)\n",
    "    #for el in doc['journal_occs']:\n",
    "    #    print(el)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
