{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pprint, json, pickle\n",
    "blacklist = ['http://www.dnb.de/DE/Service/DigitaleDienste/EntityFacts/entityfacts_node.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pools(query_results, blacklist):\n",
    "    \"\"\"\n",
    "    BEHAVIOR\n",
    "    Generates sets of URIs connected via an owl:sameAs statement \n",
    "    INPUT\n",
    "    query_results: set of triples s, sameas, same from the SPARQL query\n",
    "    blacklist: list of URIs to discard from the input\n",
    "    OUTPUT\n",
    "    A list of sets, each set is a pool of URIs connected via an owl:sameAs statement\n",
    "    \"\"\"\n",
    "    print(\"Generating pools...\", end=\" \")\n",
    "    e_index = {} # dict {resource_uri :index of the pool}\n",
    "    pools = [] # list of sets\n",
    "    pool_i = 0\n",
    "    \n",
    "    for res in results['results']['bindings']:\n",
    "        s = res['s']['value']\n",
    "        same = res['same']['value']\n",
    "    \n",
    "        if s not in blacklist and same not in blacklist:\n",
    "            if s in e_index and same not in e_index: # update existing pool\n",
    "                pools[e_index[s]].add(same)\n",
    "                e_index[same] = e_index[s]\n",
    "            elif same in e_index and s not in e_index: # update existing pool\n",
    "                pools[e_index[same]].add(s)\n",
    "                e_index[s] = e_index[same]\n",
    "            elif same not in e_index and s not in e_index: # create new pool\n",
    "                pools.append({s, same}) \n",
    "                e_index[s] = pool_i\n",
    "                e_index[same] = pool_i\n",
    "                pool_i += 1\n",
    "    print(\"Done! Generated {} pools of resource URIs from Judaica Link.\".format(len(pools)))\n",
    "    return pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pools_to_be_merged(pools):\n",
    "    \"\"\"\n",
    "    BEHAVIOR\n",
    "    Checks for overlapping pools of entities in the generated pools\n",
    "    INPUT\n",
    "    pools: a list of sets of URIs\n",
    "    OUTPUT\n",
    "    to_be_merged: list of tuples, each tuple contains indexes of pools that belong together\n",
    "    touched_pools: set of integers, indexes of all pools that are concerned by overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Processing {} pools of URIs for extracting overlaps...\".format(len(pools)))\n",
    "    uri2pool = {}\n",
    "\n",
    "    for pool in pools:\n",
    "        pool_index = pools.index(pool)\n",
    "        for uri in pool:\n",
    "            if uri in uri2pool:\n",
    "                uri2pool[uri].add(pool_index)\n",
    "            else:\n",
    "                uri2pool[uri] = {pool_index}\n",
    "    \n",
    "    uri2pool = {k: sorted(list(v)) for k,v in uri2pool.items()}\n",
    "    \n",
    "    to_be_merged = set([])\n",
    "    touched_pools = set([])\n",
    "    for pool_indexes in uri2pool.values():\n",
    "        if len(pool_indexes) > 1: # means that an URI belongs to more than one pool\n",
    "            to_be_merged.add(tuple(pool_indexes))\n",
    "            for pool_index in pool_indexes:\n",
    "                touched_pools.add(pool_index)\n",
    "    print(\"Found {} sets of pools to merge...\".format(len(to_be_merged)))\n",
    "    return (to_be_merged, touched_pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pools(pools, to_be_merged, touched_pools):\n",
    "    \"\"\"\n",
    "    BEHAVIOR\n",
    "    Merges pools of URIs that overlap (that have some URI in common)\n",
    "    INPUT\n",
    "    pools: list of sets, each is a pool of URIs\n",
    "    to_be_merged: list of tuples, each tuple contains indexes of pools that belong together\n",
    "    touched_pools: set of integers, indexes of all pools that are concerned by overlapping\n",
    "    OUTPUT\n",
    "    pools_merged: list of sets\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(to_be_merged) == 0:\n",
    "        print(\"No pools to merge! Returning pools.\")\n",
    "        return pools\n",
    "    else:\n",
    "        print(\"Merging pools...\")\n",
    "        print(\"Initial number of pools: {}\".format(len(pools)))\n",
    "        pools_merged = [pool for pool in pools if pools.index(pool) not in touched_pools]\n",
    "        for pool_indexes in to_be_merged:\n",
    "            new_pool = set([])\n",
    "            for i in pool_indexes:\n",
    "                new_pool.update(pools[i])\n",
    "            pools_merged.append(new_pool)\n",
    "        print(\"Final number of pools: {}\".format(len(pools_merged)))\n",
    "        return pools_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_pages(pools, save=False, pathout=\"\"):\n",
    "    \"\"\"\n",
    "    BEHAVIOR\n",
    "    Generates URIs for entity pages\n",
    "    INPUT\n",
    "    pools: list of sets\n",
    "    OUTPUT:\n",
    "    entity_pages: dict, {entity_page_URI: set([resource_URI_1, resource_URI_2, etc])}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating entity pages out of {} pools of entities...\".format(len(pools)))\n",
    "    entity_pages = {}\n",
    "    base = 'http://data.judaicalink.org/data/ep/'\n",
    "    i = 1000000\n",
    "    for pool in pools:\n",
    "        entity_page_uri = base+str(i)\n",
    "        entity_pages[entity_page_uri] = list(pool)\n",
    "        i += 1    \n",
    "    print(\"...Done! Generated {} entity pages!\".format(len(entity_pages)))\n",
    "    if save is True:\n",
    "        print(\"Saving entity pages to {}...\".format(pathout))\n",
    "        with open(pathout, 'wb') as outfile:\n",
    "            pickle.dump(entity_pages, outfile)\n",
    "    \n",
    "    return entity_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_inverted_index(entity_pages, save=False, pathout=\"\"):\n",
    "    \"\"\"\n",
    "    BEHAVIOR\n",
    "    Generates an inverted index {resource_URI: entity_page_URI}\n",
    "    Checks the consistency of the entity pages, each resource_URI must have exactly on entity_page_URI\n",
    "    INPUT\n",
    "    entity_pages: entity page dictionary\n",
    "    OUTPUT\n",
    "    uri2ep: dict, {resource_URI: entity_page_URI}\n",
    "    \"\"\"\n",
    "    print(\"Generating inverted index resource-to-entity_page...\")\n",
    "    uri2ep = {}\n",
    "    for ep, uris in entity_pages.items():\n",
    "        for uri in uris:\n",
    "            if uri not in uri2ep:\n",
    "                uri2ep[uri] = set([ep])\n",
    "            else:\n",
    "                uri2ep[uri].add(ep)\n",
    "    \n",
    "    print(\"Checking consistency of inverted index...\")\n",
    "    inconsistencies = 0\n",
    "    for uri, ep in uri2ep.items():\n",
    "        if len(ep) > 1:\n",
    "            inconsistencies += 1\n",
    "    if inconsistencies != 0:\n",
    "        print(\"WARNING! Found {} inconsistent entity pages!\".format(inconsistencies))\n",
    "    else:\n",
    "        print(\"The index is consistent! :)\")\n",
    "    \n",
    "    uri2ep = {k: list(v)[0] for k,v in uri2ep.items()}\n",
    "    if save is True:\n",
    "        print(\"Saving inverted index to {}...\".format(pathout))\n",
    "        with open(pathout, 'wb') as outfile:\n",
    "            pickle.dump(uri2ep, outfile)\n",
    "            \n",
    "    print(\"Done!\")\n",
    "    return uri2ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://data.judaicalink.org/sparql/query\") # changed from old:http://localhost:3040/judaicalink/query\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    SELECT ?s ?same\n",
    "    WHERE { GRAPH ?g {\n",
    "        ?s owl:sameAs ?same\n",
    "        }}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = generate_pools(results, blacklist)\n",
    "print(pools)\n",
    "to_merge, touched_pools = get_pools_to_be_merged(pools)\n",
    "merged = merge_pools(pools, to_merge, touched_pools)\n",
    "entity_pages = generate_entity_pages(merged, save=False, pathout='~/jl/output/entity_pages/ep_data.pickle')\n",
    "print(entity_pages)\n",
    "ep_inverted_index = get_ep_inverted_index(entity_pages, save=False, pathout='~/jl/output/entity_pages/ep_inverted_index.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
