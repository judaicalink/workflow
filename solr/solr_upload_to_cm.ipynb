{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc5c0bb-897e-458e-881b-e6324696b12b",
   "metadata": {},
   "source": [
    "# Load all Compact Memory Text to SOLR Core \"cm\"\n",
    "\n",
    "Requires JSON formatted files, not the txt files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedc35ec-7211-4230-bebe-642fe196b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pysolr, requests, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af355c85-7c06-4e07-b27d-c1a7a1455a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLR_URL = \"http://localhost:8983/solr\"\n",
    "CORE_NAME = \"cm\"\n",
    "CHUNK_SIZE = 200\n",
    "CONFIG_SET = \"_default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77ca700-aa22-46fb-a6c6-45b4711428e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solr_core_exists(solr_url: str, core_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a given Solr core exists using the Core Admin STATUS action.\n",
    "    \"\"\"\n",
    "    status_url = f\"{solr_url}/admin/cores\"\n",
    "    params = {\n",
    "        \"action\": \"STATUS\",\n",
    "        \"core\": core_name,\n",
    "        \"wt\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(status_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # If the core exists, it should appear in the \"status\" dict with some content\n",
    "    status = data.get(\"status\", {})\n",
    "    return core_name in status and bool(status[core_name])\n",
    "\n",
    "\n",
    "def solr_delete_core(solr_url: str, core_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Delete (unload) a Solr core and remove its index, data and instance directory.\n",
    "    \"\"\"\n",
    "    unload_url = f\"{solr_url}/admin/cores\"\n",
    "    params = {\n",
    "        \"action\": \"UNLOAD\",\n",
    "        \"core\": core_name,\n",
    "        \"deleteIndex\": \"true\",\n",
    "        \"deleteDataDir\": \"true\",\n",
    "        \"deleteInstanceDir\": \"true\",\n",
    "        \"wt\": \"json\",\n",
    "    }\n",
    "\n",
    "    print(f\"Unloading and deleting core '{core_name}' …\")\n",
    "    response = requests.get(unload_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Core '{core_name}' successfully unloaded and deleted.\")\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "def solr_create_core(solr_url: str, core_name: str, config_set: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a Solr core using a given configSet.\n",
    "    \"\"\"\n",
    "    create_url = f\"{solr_url}/admin/cores\"\n",
    "    params = {\n",
    "        \"action\": \"CREATE\",\n",
    "        \"name\": core_name,\n",
    "        \"configSet\": config_set,\n",
    "        \"wt\": \"json\",\n",
    "    }\n",
    "\n",
    "    print(f\"Creating core '{core_name}' with configSet '{config_set}' …\")\n",
    "    response = requests.get(create_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Core '{core_name}' successfully created.\")\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "def recreate_solr_core(solr_url: str, core_name: str, config_set: str) -> None:\n",
    "    \"\"\"\n",
    "    Check if the core exists, delete it if necessary, and create it again.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if solr_core_exists(solr_url, core_name):\n",
    "            print(f\"Core '{core_name}' already exists.\")\n",
    "            solr_delete_core(solr_url, core_name)\n",
    "            # Small delay to give Solr time to clean up\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print(f\"Core '{core_name}' does not exist yet.\")\n",
    "\n",
    "        solr_create_core(solr_url, core_name, config_set)\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Error while communicating with Solr:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb6f48b-2375-47d9-a1fe-85a590459c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core 'cm' already exists.\n",
      "Unloading and deleting core 'cm' …\n",
      "Core 'cm' successfully unloaded and deleted.\n",
      "{'responseHeader': {'status': 0, 'QTime': 1743}}\n",
      "Creating core 'cm' with configSet '_default' …\n",
      "Core 'cm' successfully created.\n",
      "{'responseHeader': {'status': 0, 'QTime': 346}, 'core': 'cm'}\n"
     ]
    }
   ],
   "source": [
    "# --- Run the logic ---\n",
    "recreate_solr_core(SOLR_URL, CORE_NAME, CONFIG_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3173df7b-c180-4aa2-9c19-711f1a80c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "solr = pysolr.Solr(f\"{SOLR_URL}/{CORE_NAME}\", timeout=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3713bb75-f620-4b76-826c-2bb9bf4705a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_json_files(path):\n",
    "    \"\"\"Yield JSON files recursively.\"\"\"\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".jsonl\"):\n",
    "                yield os.path.join(root, f)\n",
    "\n",
    "\n",
    "def load_docs_from_file(path):\n",
    "    \"\"\"\n",
    "    Load documents from a JSON file.\n",
    "\n",
    "    Supports:\n",
    "    - NDJSON: one JSON object per line\n",
    "    - A single JSON object\n",
    "    - A JSON array of objects\n",
    "\n",
    "    Skips broken JSON lines and prints a warning.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        content = f.read().strip()\n",
    "\n",
    "    if not content:\n",
    "        return docs\n",
    "\n",
    "    # Heuristic: if there are multiple lines and it does NOT start with '[' or '{',\n",
    "    # we treat it as NDJSON. But logstash-NDJSON ist meist: one JSON per line.\n",
    "    lines = content.splitlines()\n",
    "    if len(lines) > 1:\n",
    "        # Try NDJSON first\n",
    "        ndjson_ok = True\n",
    "        for lineno, line in enumerate(lines, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                ndjson_ok = False\n",
    "                # If we break early, we will try full-file JSON parsing below\n",
    "                break\n",
    "\n",
    "            if isinstance(obj, dict):\n",
    "                docs.append(obj)\n",
    "            elif isinstance(obj, list):\n",
    "                docs.extend(obj)\n",
    "            else:\n",
    "                # unsupported top-level type\n",
    "                pass\n",
    "\n",
    "        if ndjson_ok and docs:\n",
    "            return docs\n",
    "        # if NDJSON failed, fall through and try full JSON\n",
    "\n",
    "    # Fallback: treat entire file as a single JSON document\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[WARN] Could not parse JSON file {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    elif isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"[WARN] Unexpected top-level JSON type in {path}: {type(data)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def upload_docs(docs):\n",
    "    \"\"\"Upload a chunk of docs to Solr.\"\"\"\n",
    "    if not docs:\n",
    "        return\n",
    "    solr.add(docs, commit=False)\n",
    "\n",
    "\n",
    "def bulk_index_directory(path):\n",
    "    \"\"\"\n",
    "    Recursively scan `path` for JSON files and bulk index them into Solr.\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    for file in iter_json_files(path):\n",
    "        #print(f\"Reading {file} …\")\n",
    "        docs = load_docs_from_file(file)\n",
    "        #print(f\"  -> {len(docs)} docs parsed\")\n",
    "\n",
    "        for doc in docs:\n",
    "            buffer.append(doc)\n",
    "            if len(buffer) >= CHUNK_SIZE:\n",
    "                #print(f\"Sending chunk of {len(buffer)} docs to Solr …\")\n",
    "                upload_docs(buffer)\n",
    "                buffer = []\n",
    "\n",
    "    # last chunk\n",
    "    if buffer:\n",
    "        #print(f\"Sending final chunk of {len(buffer)} docs to Solr …\")\n",
    "        upload_docs(buffer)\n",
    "\n",
    "    solr.commit()\n",
    "    print(\"Finished import.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7991ab1-ab68-4bb0-be61-417aa11b78b3",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fba92d-53fb-4471-8b2c-1a1020d57b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished import.\n"
     ]
    }
   ],
   "source": [
    "bulk_index_directory(\"/data/cm/output/solr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90212a-dd7c-476d-b64e-9d01e3094cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
